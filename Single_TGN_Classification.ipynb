{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_unpickle_timestamp() takes exactly 3 positional arguments (4 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 250\u001b[0m\n\u001b[0;32m    248\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    249\u001b[0m train_loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xs, edge_indices, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    251\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    252\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(xs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), edge_indices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32mc:\\Python3.11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Python3.11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Python3.11\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Python3.11\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m, in \u001b[0;36mSnapshotSequenceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 53\u001b[0m         G \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# NetworkX 그래프를 PyG Data 객체로 변환\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     data \u001b[38;5;241m=\u001b[39m from_networkx(G)\n",
      "File \u001b[1;32mc:\\Python3.11\\Lib\\site-packages\\pandas\\_libs\\tslibs\\timestamps.pyx:132\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps._unpickle_timestamp\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: _unpickle_timestamp() takes exactly 3 positional arguments (4 given)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0. 프로젝트 루트 경로 추가 (필요한 경우)\n",
    "# ----------------------------------------------------------------------\n",
    "project_root = os.path.abspath(\".\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. TGN 모델 불러오기\n",
    "# ----------------------------------------------------------------------\n",
    "# 로컬에 복사한 TGN 코드는 model/tgn.py에 있습니다.\n",
    "try:\n",
    "    from model.tgn import TGN\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"TGN 모델을 불러오지 못했습니다. model/tgn.py 파일의 경로를 확인하세요.\") from e\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. SnapshotSequenceDataset: 스냅샷 파일(.gpickle)로부터 동적 그래프 시퀀스 구성\n",
    "# ----------------------------------------------------------------------\n",
    "class SnapshotSequenceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Snapshots 폴더 내의 모든 .gpickle 파일을 시간 순서대로 불러와,\n",
    "    하나의 동적 그래프 시퀀스로 구성합니다.\n",
    "    train 데이터는 label 정보를 포함하며, \n",
    "    test 데이터는 평가를 위해 ground truth label이 있으나, 스냅샷 생성 시에는 (drop_label 옵션으로)\n",
    "    그래프 feature에서 label이 제거될 수 있습니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, snapshot_folder, label=None):\n",
    "        self.snapshot_folder = snapshot_folder\n",
    "        self.files = sorted([os.path.join(snapshot_folder, f) \n",
    "                             for f in os.listdir(snapshot_folder) if f.endswith('.gpickle')])\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        # 전체 시퀀스를 하나의 샘플로 취급합니다.\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_x = []           # 각 스냅샷의 노드 feature 텐서\n",
    "        sequence_edge_index = []  # 각 스냅샷의 edge_index 텐서\n",
    "        for file in self.files:\n",
    "            with open(file, 'rb') as f:\n",
    "                G = pickle.load(f)\n",
    "            # NetworkX 그래프를 PyG Data 객체로 변환\n",
    "            data = from_networkx(G)\n",
    "            # 노드 feature가 없으면, 각 노드의 degree를 feature로 사용 (1차원)\n",
    "            if not hasattr(data, 'x') or data.x is None:\n",
    "                deg = torch.tensor([val for (_, val) in G.degree()], dtype=torch.float).unsqueeze(1)\n",
    "                data.x = deg\n",
    "            sequence_x.append(data.x)\n",
    "            sequence_edge_index.append(data.edge_index)\n",
    "        return sequence_x, sequence_edge_index, self.label\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. 스냅샷 생성 함수들\n",
    "# ----------------------------------------------------------------------\n",
    "def create_graph_from_snapshot(snapshot_df, drop_label=False):\n",
    "    \"\"\"\n",
    "    snapshot_df의 각 행(패킷)을 하나의 통신으로 보고,\n",
    "    'wlan.sa'와 'wlan.da'를 노드로 추가하며,\n",
    "    나머지 feature (전체 feature 목록 중 'wlan.sa', 'wlan.da' 제외)를 엣지 속성으로 저장하는 그래프를 생성합니다.\n",
    "    \n",
    "    drop_label이 True이면, 각 패킷 정보 딕셔너리에서 'label' 키를 제거합니다.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for idx, row in snapshot_df.iterrows():\n",
    "        src = row.get('wlan.sa')\n",
    "        dst = row.get('wlan.da')\n",
    "        if pd.isna(src) or pd.isna(dst):\n",
    "            continue\n",
    "        packet_info = row.to_dict()\n",
    "        packet_info.pop('wlan.sa', None)\n",
    "        packet_info.pop('wlan.da', None)\n",
    "        if drop_label:\n",
    "            packet_info.pop('label', None)\n",
    "        # Timestamp 타입이 있으면 float(timestamp)로 변환\n",
    "        for key, value in packet_info.items():\n",
    "            if isinstance(value, (np.datetime64, torch.Tensor)) or hasattr(value, 'timestamp'):\n",
    "                try:\n",
    "                    packet_info[key] = value.timestamp()\n",
    "                except Exception:\n",
    "                    packet_info[key] = float(value)\n",
    "        if src not in G:\n",
    "            G.add_node(src, type='device')\n",
    "        if dst not in G:\n",
    "            G.add_node(dst, type='device')\n",
    "        if G.has_edge(src, dst):\n",
    "            G[src][dst]['count'] += 1\n",
    "            G[src][dst]['features'].append(packet_info)\n",
    "        else:\n",
    "            G.add_edge(src, dst, count=1, features=[packet_info])\n",
    "    return G\n",
    "\n",
    "def save_graph_snapshot(G, snapshot_time, attack_name, dataset_type=\"train\"):\n",
    "    \"\"\"\n",
    "    그래프 G를 pickle 파일로 저장합니다.\n",
    "    파일명에 공격 이름과 snapshot_time(YYYYMMDD_HHMMSS)을 포함하며,\n",
    "    dataset_type (\"train\" 또는 \"test\")에 따라 다른 폴더에 저장합니다.\n",
    "    \"\"\"\n",
    "    time_str = snapshot_time.strftime('%Y%m%d_%H%M%S')\n",
    "    file_name = f\"{attack_name}_snapshot_{time_str}.gpickle\"\n",
    "    if dataset_type == \"train\":\n",
    "        file_path = os.path.join(os.getcwd(), \"Snapshots\", \"train\", attack_name, file_name)\n",
    "    else:\n",
    "        file_path = os.path.join(os.getcwd(), \"Snapshots\", \"test\", attack_name, file_name)\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(G, f)\n",
    "    print(f\"Saved {dataset_type} snapshot: {file_path}\")\n",
    "\n",
    "def generate_dynamic_snapshots(df, attack_name, time_column='frame.time', interval='5min', drop_label=False):\n",
    "    \"\"\"\n",
    "    DataFrame의 'frame.time' 열(에포크 초 값)을 기준으로,\n",
    "    각 스냅샷을 처음부터 해당 시점까지의 누적 통신 데이터를 기반으로 생성합니다.\n",
    "    생성된 스냅샷은 전체 통신 내역의 누적 그래프(NetworkX 객체)이며,\n",
    "    drop_label이 True이면, 각 패킷 정보에서 'label'이 제거됩니다.\n",
    "    \"\"\"\n",
    "    df[time_column] = pd.to_datetime(df[time_column], unit='s')\n",
    "    df = df.sort_values(time_column)\n",
    "    start_time = df[time_column].min()\n",
    "    end_time = df[time_column].max()\n",
    "    snapshots = {}\n",
    "    current_time = start_time\n",
    "    print(f\"Creating cumulative snapshots for {attack_name} from {start_time} to {end_time} with interval {interval}.\")\n",
    "    while current_time < end_time:\n",
    "        next_time = current_time + pd.Timedelta(interval)\n",
    "        snapshot_df = df[df[time_column] < next_time]\n",
    "        if not snapshot_df.empty:\n",
    "            G_snapshot = create_graph_from_snapshot(snapshot_df, drop_label=drop_label)\n",
    "            snapshots[current_time] = G_snapshot\n",
    "            print(f\"Cumulative Snapshot up to {next_time.strftime('%Y-%m-%d %H:%M:%S')} - {G_snapshot.number_of_nodes()} nodes, {G_snapshot.number_of_edges()} edges\")\n",
    "            dtype = \"train\" if not drop_label else \"test\"\n",
    "            save_graph_snapshot(G_snapshot, current_time, attack_name, dataset_type=dtype)\n",
    "        current_time = next_time\n",
    "    print(\"All cumulative snapshots created.\")\n",
    "    return snapshots\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. TGNClassifier 정의 (TGN 모델 래핑)\n",
    "# ----------------------------------------------------------------------\n",
    "class TGNClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, tgn_args):\n",
    "        \"\"\"\n",
    "        in_channels: 입력 노드 feature 차원 (예: 1)\n",
    "        hidden_channels: TGN 내부 hidden dimension\n",
    "        out_channels: 분류할 클래스 수 (예: 14)\n",
    "        tgn_args: TGN 초기화에 필요한 인자들을 담은 dict (neighbor_finder, node_features, edge_features, device 등)\n",
    "        \"\"\"\n",
    "        super(TGNClassifier, self).__init__()\n",
    "        self.tgn = TGN(**tgn_args)\n",
    "        self.linear = nn.Linear(hidden_channels, out_channels)\n",
    "    \n",
    "    def forward(self, xs, edge_indices):\n",
    "        h = None\n",
    "        for x, edge_index in zip(xs, edge_indices):\n",
    "            if edge_index.size(1) > 0:\n",
    "                edge_attr = torch.ones((edge_index.size(1), 1), device=x.device)\n",
    "                t_val = torch.zeros(edge_index.size(1), device=x.device)\n",
    "            else:\n",
    "                edge_attr = None\n",
    "                t_val = None\n",
    "            h = self.tgn(x, edge_index, edge_attr, t_val)\n",
    "        h_mean = h.mean(dim=0)\n",
    "        out = self.linear(h_mean)\n",
    "        return out\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5. 학습 및 검증 설정\n",
    "# ----------------------------------------------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "in_channels = 1       # 예: 노드 feature가 degree이면 1\n",
    "hidden_channels = 32\n",
    "num_classes = 14      # 다중 분류: 14 클래스 (전처리 시 label_mapping 기준)\n",
    "\n",
    "# tgn_args: TGN 초기화에 필요한 인자들 (예시 더미 값)\n",
    "n_nodes = 1000\n",
    "node_feat_dim = 10\n",
    "edge_feat_dim = 5\n",
    "# 더미 neighbor finder (실제 구현에서는 저장소 내 neighbor_finder 사용)\n",
    "class DummyNeighborFinder:\n",
    "    def __init__(self, n_nodes):\n",
    "        self.n_nodes = n_nodes\n",
    "    def find_before(self, node, timestamp, n_neighbors=20):\n",
    "        return np.arange(min(n_neighbors, self.n_nodes))\n",
    "dummy_neighbor_finder = DummyNeighborFinder(n_nodes)\n",
    "\n",
    "tgn_args = {\n",
    "    'neighbor_finder': dummy_neighbor_finder,\n",
    "    'node_features': np.random.randn(n_nodes, node_feat_dim).astype(np.float32),\n",
    "    'edge_features': np.random.randn(5000, edge_feat_dim).astype(np.float32),\n",
    "    'device': device,\n",
    "    'n_layers': 2,\n",
    "    'n_heads': 2,\n",
    "    'dropout': 0.1,\n",
    "    'use_memory': True,\n",
    "    'memory_update_at_start': True,\n",
    "    'message_dimension': 100,\n",
    "    'memory_dimension': 500,\n",
    "    'embedding_module_type': \"graph_attention\",\n",
    "    'message_function': \"mlp\",\n",
    "    'mean_time_shift_src': 0,\n",
    "    'std_time_shift_src': 1,\n",
    "    'mean_time_shift_dst': 0,\n",
    "    'std_time_shift_dst': 1,\n",
    "    'n_neighbors': 20,\n",
    "    'aggregator_type': \"last\",\n",
    "    'memory_updater_type': \"gru\",\n",
    "    'use_destination_embedding_in_message': False,\n",
    "    'use_source_embedding_in_message': False,\n",
    "    'dyrep': False\n",
    "}\n",
    "\n",
    "model = TGNClassifier(in_channels, hidden_channels, num_classes, tgn_args).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 스냅샷 폴더 경로 설정 (예: Deauth 공격)\n",
    "TRAIN_SNAPSHOT_FOLDER = os.path.join(os.getcwd(), \"Snapshots\", \"train\", \"Deauth\")\n",
    "TEST_SNAPSHOT_FOLDER  = os.path.join(os.getcwd(), \"Snapshots\", \"test\", \"Deauth\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6. Snapshot 데이터셋 및 DataLoader 구성\n",
    "# ----------------------------------------------------------------------\n",
    "train_dataset = SnapshotSequenceDataset(TRAIN_SNAPSHOT_FOLDER, label=1)  # 예시: Deauth가 1번\n",
    "test_dataset  = SnapshotSequenceDataset(TEST_SNAPSHOT_FOLDER, label=1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 7. 학습 루프 (Train) - Train Loss, Test Loss, Test Accuracy 출력\n",
    "# ----------------------------------------------------------------------\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss_epoch = 0\n",
    "    for xs, edge_indices, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(xs[0].to(device), edge_indices[0].to(device))\n",
    "        loss = criterion(output.unsqueeze(0), torch.tensor([label], dtype=torch.long, device=device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += loss.item()\n",
    "    train_loss_epoch /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss_epoch = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xs, edge_indices, label in test_loader:\n",
    "            output = model(xs[0].to(device), edge_indices[0].to(device))\n",
    "            loss = criterion(output.unsqueeze(0), torch.tensor([label], dtype=torch.long, device=device))\n",
    "            test_loss_epoch += loss.item()\n",
    "            pred = output.argmax(dim=-1)\n",
    "            correct += (pred.item() == label)\n",
    "            total += 1\n",
    "    test_loss_epoch /= len(test_loader)\n",
    "    test_acc = correct / total if total > 0 else 0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss_epoch:.4f}, Test Loss: {test_loss_epoch:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 8. 최종 검증 (Test)\n",
    "# ----------------------------------------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xs, edge_indices, label in test_loader:\n",
    "         output = model(xs[0].to(device), edge_indices[0].to(device))\n",
    "         pred = output.argmax(dim=-1)\n",
    "         print(\"Final Test Prediction:\", pred.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
